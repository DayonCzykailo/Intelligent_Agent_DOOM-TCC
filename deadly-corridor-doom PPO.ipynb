{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892256f8-e51f-40a5-a233-140f6a6170a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/njustesen/rarity-of-events/blob/master/heatmap.py\n",
    "#https://nolanwinsman.com/pdfs/VizDoom-Paper.pdf -- img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecec630-4ef4-4faa-aba2-127b0ecc4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#![title](img/maps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e9d88f-7c51-4593-aadb-0afb1fedc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9d53e1-2498-4f20-bc19-457717f45a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd github & git clone https://github.com/Farama-Foundation/ViZDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479f5803-e444-461c-a779-e16af718cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vizdoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# Import numpy for identity matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a67633-4e5a-4057-a4ed-24a24e9d6270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"Device name:\", torch.cuda.get_device_name(device=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "312313f7-06a0-4288-afdf-1481c9ed5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup game\n",
    "# game = DoomGame()\n",
    "# game.load_config('scenarios/deadly_corridor.cfg')\n",
    "# game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913ceefa-9dd3-44ec-a688-3ce675ca812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is the set of actions we can take in the environment\n",
    "# actions = np.identity(7, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf8e40b9-2318-4804-b926-e1201d57955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = game.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf802286-612b-4342-b4a6-bd41690229b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state.game_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186adeba-1ba1-4074-875f-1fcc3e937d3d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Loop through episodes\n",
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    # Create a new episode or game\n",
    "    game.new_episode()\n",
    "    # Check the game isn't done\n",
    "    while not game.is_episode_finished():\n",
    "        # Get the game state\n",
    "        state = game.get_state()\n",
    "        # Get the game image\n",
    "        img = state.screen_buffer\n",
    "        # Get the game variables - ammo\n",
    "        info = state.game_variables\n",
    "        # Take an action\n",
    "        reward = game.make_action(random.choice(actions), 4)\n",
    "        # Print reward \n",
    "        # print('reward: ', reward)\n",
    "        time.sleep(0.02)\n",
    "    print('Result: ', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1398a4d0-9d02-4939-84da-c0d761d290b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a0ce9-6f22-4ff4-b44e-b4fa3a3e811e",
   "metadata": {},
   "source": [
    "## 2. Converting it to a Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645c1496-b01d-44f2-a5b0-941d94049c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cdeb278-b54d-4d0b-bbc6-f03533670443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "609be78f-45b2-4175-90a5-ef47903b1180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gym import Env\n",
    "# Import gym spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "# Import opencv\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fdb66fd-ea11-43bf-b4f8-03cd864ca98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Vizdoom OpenAI Gym Environment\n",
    "# class VizDoomGym(Env):\n",
    "#     # Function that is called when we start the env\n",
    "# #    def __init__(self, render=False, config='github/ViZDoom/scenarios/deadly_corridor_s1.cfg'):\n",
    "#     def __init__(self, render=False, config='scenarios/deadly_corridor.cfg'):\n",
    "#         # Inherit from Env\n",
    "#         super().__init__()\n",
    "#         # Setup the game\n",
    "#         self.game = DoomGame()\n",
    "#         self.game.load_config(config)\n",
    "\n",
    "#         # Render frame logic\n",
    "#         if render == False:\n",
    "#             self.game.set_window_visible(False)\n",
    "#         else:\n",
    "#             self.game.set_window_visible(True)\n",
    "\n",
    "#         # Start the game\n",
    "#         self.game.init()\n",
    "\n",
    "#         # Create the action space and observation space\n",
    "#         self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "#         self.action_space = Discrete(7)\n",
    "\n",
    "#         # Game variables: HEALTH DAMAGE_TAKEN HITCOUNT SELECTED_WEAPON_AMMO\n",
    "#         self.last_health = 100\n",
    "#         self.last_ammo = 52 \n",
    "#         self.last_hitcount = 0\n",
    "    \n",
    "#     # This is how we take a step in the environment\n",
    "#     def step(self, action):\n",
    "#         # Specify action and take step\n",
    "#         actions = np.identity(7)\n",
    "#         movement_reward = self.game.make_action(actions[action], 4) # 4 here is the frameskip parameter which makes the game run faster\n",
    "\n",
    "#         reward = 0\n",
    "\n",
    "#         # Get all the other stuff we need to return\n",
    "#         if self.game.get_state():\n",
    "#             state = self.game.get_state().screen_buffer\n",
    "#             state = self.grayscale(state)\n",
    "\n",
    "#             # Reward shaping\n",
    "#             # game_variables = self.game.get_state().game_variables\n",
    "#             # health, damage_taken, hitcount, ammo = game_variables\n",
    "\n",
    "\n",
    "#             # damage_taken_delta = self.damage_taken - damage_taken\n",
    "#             # self.damage_taken = damage_taken\n",
    "#             # hit_count_delta = hit_count - self.hit_count\n",
    "#             # self.hit_count = hit_count\n",
    "#             # ammo_delta = ammo - self.selected_weapon_ammo\n",
    "#             # self.selected_weapon_ammo = ammo\n",
    "\n",
    "#             # reward = movement_reward + damage_taken_delta*10 + hit_count_delta*200 + ammo_delta*5\n",
    "\n",
    "#             # if self.hitcount != hitcount:\n",
    "#             #     reward += ((hitcount - self.hitcount) * 200)\n",
    "\n",
    "#             # if self.last_health != health:\n",
    "#             #     reward += -((self.last_health - health) * 10)\n",
    "            \n",
    "#             # if self.last_ammo != ammo:# negatively reinforcing ammo\n",
    "#             #     reward += -10\n",
    "\n",
    "\n",
    "                \n",
    "#             # self.last_ammo = ammo\n",
    "#             # self.last_health = health\n",
    "#             # self.hitcount = hitcount\n",
    "\n",
    "#             # reward = movement_reward + damage_taken_delta*10 + hitcount_delta*200 + ammo_delta*5\n",
    "#             # reward += movement_reward\n",
    "#             #reward = (reward / 100)\n",
    "\n",
    "#             reward = movement_reward\n",
    "\n",
    "            \n",
    "#             #info = ammo\n",
    "#             info = 0\n",
    "#         else:\n",
    "#             state = np.zeros(self.observation_space.shape)\n",
    "#             info = 0\n",
    "\n",
    "#         info = {'info': info}\n",
    "#         done = self.game.is_episode_finished()\n",
    "\n",
    "#         return state, reward, done, info\n",
    "        \n",
    "#     # Define how to render the game or environment\n",
    "#     def render():\n",
    "#         pass\n",
    "\n",
    "#     # What happens when we start a new game\n",
    "#     def reset(self):\n",
    "#         # It is considered best practice to reset variables, so trying this out as well! :3 \n",
    "#         self.damage_taken = 0\n",
    "#         self.hitcount = 0\n",
    "#         self.last_ammo = 52\n",
    "        \n",
    "#         self.game.new_episode()\n",
    "#         state = self.game.get_state().screen_buffer\n",
    "#         return self.grayscale(state)\n",
    "\n",
    "#     # Grayscale the game frame and resize it \n",
    "#     def grayscale(self, observation):\n",
    "#         gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "#         resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "#         state = np.reshape(resize, (100,160,1))\n",
    "#         return state\n",
    "\n",
    "#     # Call to close down the game\n",
    "#     def close(self):\n",
    "#         self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b7f13b9-dec7-4b22-8c64-d6d028b4b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vizdoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env):\n",
    "    # Function that is called when we start the env\n",
    "#    def __init__(self, render=False, config='github/ViZDoom/scenarios/deadly_corridor_s1.cfg'):\n",
    "    def __init__(self, render=False, config='scenarios/deadly_corridor.cfg'):\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        # Setup the game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "\n",
    "        # Render frame logic\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "\n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7)\n",
    "\n",
    "        # Game variables: HEALTH DAMAGE_TAKEN HITCOUNT SELECTED_WEAPON_AMMO\n",
    "        self.last_health = 100\n",
    "        self.last_armor = 0\n",
    "        self.last_damage_taken = 0\n",
    "        self.last_hit_taken = 0\n",
    "        self.last_damagecount = 0\n",
    "        self.last_hitcount = 0\n",
    "        self.last_killcount = 0\n",
    "        self.last_selected_weapon_ammo = 52\n",
    "        self.dead = 0\n",
    "    \n",
    "    # This is how we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(7)\n",
    "        movement_reward = self.game.make_action(actions[action], 4) # 4 here is the frameskip parameter which makes the game run faster\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # HEALTH\n",
    "        # ARMOR\n",
    "        # DAMAGE_TAKEN\n",
    "        # HITS_TAKEN\n",
    "        # DAMAGECOUNT\n",
    "        # HITCOUNT\n",
    "        # KILLCOUNT\n",
    "        # SELECTED_WEAPON_AMMO\n",
    "        # DEAD\n",
    "        #[94.  0.  6.  1. 15.  1.  1. 50.  0.]\n",
    "        # Get all the other stuff we need to return\n",
    "        if self.game.get_state():\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "\n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables\n",
    "            health, armor, damage_taken, hit_taken, damagecount, hitcount, killcount, ammo, dead = game_variables\n",
    "\n",
    "            if self.last_hit_taken != hit_taken:\n",
    "                reward += -20 \n",
    "                reward += (-(health - self.last_health) * 10) # / 2\n",
    "\n",
    "            if self.last_killcount != killcount:\n",
    "                reward += killcount * 300\n",
    "\n",
    "            if self.last_hitcount != hitcount:\n",
    "                reward += hitcount * 100\n",
    "\n",
    "            if self.last_selected_weapon_ammo != ammo:# negatively reinforcing ammo\n",
    "                reward += -10\n",
    "            \n",
    "            reward += movement_reward\n",
    "\n",
    "            reward = reward / 100\n",
    "\n",
    "            self.last_health = health\n",
    "            self.last_armor = armor\n",
    "            self.last_damage_taken = damage_taken\n",
    "            self.last_hit_taken = hit_taken\n",
    "            self.last_damagecount = damagecount\n",
    "            self.last_hitcount = hitcount\n",
    "            self.last_killcount = killcount\n",
    "            self.last_selected_weapon_ammo = ammo\n",
    "            self.dead = dead\n",
    "            \n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "\n",
    "        info = {'info': 0}\n",
    "        done = self.game.is_episode_finished()\n",
    "\n",
    "        return state, reward, done, info\n",
    "        \n",
    "    # Define how to render the game or environment\n",
    "    def render():\n",
    "        pass\n",
    "\n",
    "    # What happens when we start a new game\n",
    "    def reset(self):\n",
    "        # It is considered best practice to reset variables, so trying this out as well! :3 \n",
    "        self.last_health = 100\n",
    "        self.last_armor = 0\n",
    "        self.last_damage_taken = 0\n",
    "        self.last_hit_taken = 0\n",
    "        self.last_damagecount = 0\n",
    "        self.last_hitcount = 0\n",
    "        self.last_killcount = 0\n",
    "        self.last_selected_weapon_ammo = 52\n",
    "        self.dead = 0\n",
    "        \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "\n",
    "    # Grayscale the game frame and resize it \n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "\n",
    "    # Call to close down the game\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8eff3ce-c39d-4476-9875-48ec5b0ab852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abf1fbdb-1993-4c59-b850-caa13f81cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afeb330d-2954-4ed3-a12d-bc59b30fb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ff39115-b98e-4e4f-9b6d-7d301f5a7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Environment checker\n",
    "# from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fa0a8d6-0941-4ae8-92c4-39ea6add22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b5413-701d-49ed-9333-a592132a59d4",
   "metadata": {},
   "source": [
    "## 3. View State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "971a8781-57b7-4a6a-9794-35056c3a05ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17531505-9517-47e8-9df6-b79779e1598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.cvtColor(state, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31e77df8-60ba-4d7c-8d58-0e53d4de05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decccc69-643b-4a85-a243-4f30d9d8cf68",
   "metadata": {},
   "source": [
    "## 4. Setup Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c2ed4ec-5b24-4a8c-b492-f9e256adcec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33f7788d-ffa1-48e9-b668-fe7580824534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05ef1fa2-4235-4b99-9c16-e146f99007a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 20:20:33.854282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import os for file nav\n",
    "import os \n",
    "# Import callback class from sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c93a52ad-10ba-468f-8ad8-3e7c59987815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cfaaa5e-c0de-4693-a619-76f68f8ec423",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/ppo/train_corridor'\n",
    "LOG_DIR = './logs/log_corridor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c421803-7838-462a-835c-96ac34e9647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a22300-9c23-4cb4-adf4-0ec0a1371adb",
   "metadata": {},
   "source": [
    "## 5. Train Our Model Using Curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1d64eb2-18c7-480d-9b45-2174951a8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ppo for training \n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28b878cb-d5fb-41dd-a3dc-9f1090a91b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment\n",
    "env = VizDoomGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11f13075-1aab-42e2-a1cd-10cb7d0530c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=4096)\n",
    "\n",
    "# model = PPO(\n",
    "#     'CnnPolicy', \n",
    "#     env, \n",
    "#     tensorboard_log=LOG_DIR, \n",
    "#     verbose=1, \n",
    "#     learning_rate=0.0001,\n",
    "#     n_steps=8192,\n",
    "#     clip_range=0.1,\n",
    "#     gamma=0.95,\n",
    "#     gae_lambda=0.9\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70ae4ada-b5fd-415f-9e4f-ae92ad490728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=500000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae69bdd-0b02-4af6-9525-cd3aaabcd700",
   "metadata": {},
   "source": [
    "Tranining on Difficulty Level - 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e926742-a14e-496a-bf38-b027dea5fff1",
   "metadata": {},
   "source": [
    "## 6. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72063e00-d340-44c7-b5b2-9d3d10ae5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b28d4694-17cf-4654-a493-5a9d7c5eaeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(config='scenarios/deadly_corridor.cfg')\n",
    "# model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f183011f-1836-404d-9b30-6473aec84950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from disk\n",
    "model = PPO.load('./train/ppo/train_corridor/best_model_500000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "814614e7-b321-471e-8c27-b64435bb2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rendered environment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e638747-eff3-480b-a85e-9648103fecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dayon/Projetos/Intelligent_Agent_DOOM-TCC/.venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/dayon/Projetos/Intelligent_Agent_DOOM-TCC/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate mean reward for 5 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02b3d1ac-d23c-428d-b3b8-7458005d5240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(89.17632887593354)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4afa5-7a18-4566-a674-af90166e1860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 0 is 86.15705673217776\n",
      "Total Reward for episode 1 is 63.155278167724596\n",
      "Total Reward for episode 2 is 82.10906875610354\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.20)\n",
    "        total_reward += reward\n",
    "\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852e359-8292-4648-8cfd-8a57f2a4a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "print(f'Mean reward over 100 episodes: {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0ff99-d99c-4c01-8c7a-4b65170d44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_lengths = []\n",
    "success_count = 0\n",
    "rewards = []\n",
    "\n",
    "# Define a success threshold (e.g., 10 as an example; adjust based on your environment)\n",
    "success_threshold = 65\n",
    "\n",
    "for episode in range(100):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    episode_lengths.append(steps)\n",
    "    rewards.append(total_reward)\n",
    "    \n",
    "    # Check if the episode is successful\n",
    "    if total_reward > success_threshold:\n",
    "        success_count += 1\n",
    "\n",
    "avg_episode_length = np.mean(episode_lengths)\n",
    "success_rate = success_count / 100\n",
    "reward_variance = np.var(rewards)\n",
    "\n",
    "print(f'Average Episode Length: {avg_episode_length}')\n",
    "print(f'Success Rate: {success_rate}')\n",
    "print(f'Reward Variance (Stability): {reward_variance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995de90-851f-4424-91c6-63dd583c3d4b",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07eff1-bd57-4072-a1b2-d0787830f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d1e1e-7588-471d-88f2-93cf525df2f4",
   "metadata": {},
   "source": [
    "### Visualizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911a34c-ad6d-45e0-bcf4-421530d56669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Episode Length Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(episode_lengths, bins=10, color='blue', edgecolor='black', label='Episode Lengths')\n",
    "plt.title('Episode Length Distribution')\n",
    "plt.xlabel('Number of Steps')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')  # Add legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91851bf8-4ede-416b-9ec0-06b4a31e5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Reward Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(rewards, bins=10, color='green', edgecolor='black', label='Rewards')\n",
    "plt.title('Reward Distribution')\n",
    "plt.xlabel('Total Reward per Episode')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')  # Add legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7f34d-017b-49b0-a4c6-db37df6e4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Success Rate (Doughnut Chart)\n",
    "plt.figure(figsize=(6, 6))\n",
    "labels = ['Success', 'Failure']\n",
    "sizes = [success_rate * 100, (1 - success_rate) * 100]\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "\n",
    "# Create a doughnut chart\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, wedgeprops=dict(width=0.3))\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(labels, loc='best')\n",
    "\n",
    "plt.title('Success Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db3e01-bf1f-4ec8-8611-23be4012b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Reward Variance Over Time (Line Chart)\n",
    "reward_variances = []\n",
    "\n",
    "for episode in range(100):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    if len(rewards) > 1:\n",
    "        variance = np.var(rewards)\n",
    "        reward_variances.append(variance)\n",
    "\n",
    "# Create a line chart for reward variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(reward_variances, color='orange', marker='o', label='Reward Variance')\n",
    "plt.title('Reward Variance Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward Variance (points squared)')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')  # Add legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ebdccc-3909-4930-8c0c-0e2a28a05df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274aef1e-3f39-4cba-aafa-198681cf929b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831f7e0-21e4-4beb-aa70-86096e726aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
